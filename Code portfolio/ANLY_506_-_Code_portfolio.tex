\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={ANLY 506 - Code Portfolio},
            pdfauthor={Ly Dang},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{ANLY 506 - Code Portfolio}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Ly Dang}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{June 09, 2019}


\begin{document}
\maketitle

\subsection{I. Introduction}\label{i.-introduction}

Code portfolio summarizing weekly coding practice

\subsection{II. Code porfolio:}\label{ii.-code-porfolio}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(igraph)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'igraph'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     decompose, spectrum
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:base':
## 
##     union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:igraph':
## 
##     as_data_frame, groups, union
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

\paragraph{Exploratory Data Analysis}\label{exploratory-data-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{'readr'}\NormalTok{)}
\NormalTok{ozone <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"~/A-HU CPT/ANLY 506/US EPA data 2017.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   .default = col_character(),
##   `Parameter Code` = col_integer(),
##   POC = col_integer(),
##   Latitude = col_double(),
##   Longitude = col_double(),
##   Year = col_integer(),
##   `Observation Count` = col_integer(),
##   `Observation Percent` = col_integer(),
##   `Valid Day Count` = col_integer(),
##   `Required Day Count` = col_integer(),
##   `Exceptional Data Count` = col_integer(),
##   `Null Data Count` = col_integer(),
##   `Primary Exceedance Count` = col_integer(),
##   `Secondary Exceedance Count` = col_integer(),
##   `Num Obs Below MDL` = col_integer(),
##   `Arithmetic Mean` = col_double(),
##   `Arithmetic Standard Dev` = col_double(),
##   `1st Max Value` = col_double(),
##   `1st Max DateTime` = col_datetime(format = ""),
##   `2nd Max Value` = col_double(),
##   `2nd Max DateTime` = col_datetime(format = "")
##   # ... with 16 more columns
## )
\end{verbatim}

\begin{verbatim}
## See spec(...) for full column specifications.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(ozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    66869 obs. of  55 variables:
##  $ State Code                   : chr  "01" "01" "01" "01" ...
##  $ County Code                  : chr  "003" "003" "003" "003" ...
##  $ Site Num                     : chr  "0010" "0010" "0010" "0010" ...
##  $ Parameter Code               : int  44201 44201 44201 44201 88101 88101 88101 88101 88101 88101 ...
##  $ POC                          : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Latitude                     : num  30.5 30.5 30.5 30.5 30.5 ...
##  $ Longitude                    : num  -87.9 -87.9 -87.9 -87.9 -87.9 ...
##  $ Datum                        : chr  "NAD83" "NAD83" "NAD83" "NAD83" ...
##  $ Parameter Name               : chr  "Ozone" "Ozone" "Ozone" "Ozone" ...
##  $ Sample Duration              : chr  "1 HOUR" "8-HR RUN AVG BEGIN HOUR" "8-HR RUN AVG BEGIN HOUR" "8-HR RUN AVG BEGIN HOUR" ...
##  $ Pollutant Standard           : chr  "Ozone 1-hour 1979" "Ozone 8-Hour 1997" "Ozone 8-Hour 2008" "Ozone 8-hour 2015" ...
##  $ Metric Used                  : chr  "Daily maxima of observed hourly values (between 9:00 AM and 8:00 PM)" "Daily maximum of 8 hour running average of observed hourly values" "Daily maximum of 8 hour running average of observed hourly values" "Daily maximum of 8-hour running average" ...
##  $ Method Name                  : chr  "INSTRUMENTAL - ULTRA VIOLET ABSORPTION" NA NA NA ...
##  $ Year                         : int  2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ...
##  $ Units of Measure             : chr  "Parts per million" "Parts per million" "Parts per million" "Parts per million" ...
##  $ Event Type                   : chr  "No Events" "No Events" "No Events" "No Events" ...
##  $ Observation Count            : int  5240 5426 5426 3852 110 110 110 110 118 118 ...
##  $ Observation Percent          : int  93 91 91 90 90 90 90 90 97 97 ...
##  $ Completeness Indicator       : chr  "Y" "Y" "Y" "Y" ...
##  $ Valid Day Count              : int  227 222 222 221 110 110 110 110 118 118 ...
##  $ Required Day Count           : int  245 245 245 245 122 122 122 122 122 122 ...
##  $ Exceptional Data Count       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Null Data Count              : int  640 0 0 0 12 12 12 12 6 6 ...
##  $ Primary Exceedance Count     : int  0 0 0 1 0 0 NA NA 0 0 ...
##  $ Secondary Exceedance Count   : int  0 0 0 1 0 0 NA NA 0 0 ...
##  $ Certification Indicator      : chr  "Certified" "Certified" "Certified" "Certified" ...
##  $ Num Obs Below MDL            : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Arithmetic Mean              : num  0.0443 0.0401 0.0401 0.0399 7.3918 ...
##  $ Arithmetic Standard Dev      : num  0.0122 0.0116 0.0116 0.0116 3.5723 ...
##  $ 1st Max Value                : num  0.078 0.073 0.073 0.073 19.7 19.7 19.7 19.7 19 19 ...
##  $ 1st Max DateTime             : POSIXct, format: "2017-05-08 15:00:00" "2017-05-08 14:00:00" ...
##  $ 2nd Max Value                : num  0.078 0.07 0.07 0.07 18.9 18.9 18.9 18.9 17.4 17.4 ...
##  $ 2nd Max DateTime             : POSIXct, format: "2017-05-09 14:00:00" "2017-05-09 09:00:00" ...
##  $ 3rd Max Value                : num  0.072 0.067 0.067 0.067 18.9 18.9 18.9 18.9 16.8 16.8 ...
##  $ 3rd Max DateTime             : POSIXct, format: "2017-05-10 14:00:00" "2017-05-10 09:00:00" ...
##  $ 4th Max Value                : num  0.067 0.064 0.064 0.064 16 16 16 16 16.3 16.3 ...
##  $ 4th Max DateTime             : POSIXct, format: "2017-04-25 15:00:00" "2017-05-07 13:00:00" ...
##  $ 1st Max Non Overlapping Value: num  NA NA NA NA NA NA NA NA NA NA ...
##  $ 1st NO Max DateTime          : POSIXct, format: NA NA ...
##  $ 2nd Max Non Overlapping Value: num  NA NA NA NA NA NA NA NA NA NA ...
##  $ 2nd NO Max DateTime          : POSIXct, format: NA NA ...
##  $ 99th Percentile              : num  0.072 0.067 0.067 0.067 18.9 18.9 18.9 18.9 17.4 17.4 ...
##  $ 98th Percentile              : num  0.067 0.062 0.062 0.062 18.9 18.9 18.9 18.9 16.8 16.8 ...
##  $ 95th Percentile              : num  0.063 0.058 0.058 0.058 15.4 15.4 15.4 15.4 15 15 ...
##  $ 90th Percentile              : num  0.06 0.054 0.054 0.054 11.8 11.8 11.8 11.8 12.5 12.5 ...
##  $ 75th Percentile              : num  0.053 0.049 0.049 0.048 8.9 8.9 8.9 8.9 10.4 10.4 ...
##  $ 50th Percentile              : num  0.044 0.04 0.04 0.04 7 7 7 7 7.2 7.2 ...
##  $ 10th Percentile              : num  0.027 0.024 0.024 0.024 3.8 3.8 3.8 3.8 3.8 3.8 ...
##  $ Local Site Name              : chr  "FAIRHOPE, Alabama" "FAIRHOPE, Alabama" "FAIRHOPE, Alabama" "FAIRHOPE, Alabama" ...
##  $ Address                      : chr  "FAIRHOPE HIGH SCHOOL, 1 PIRATE DRIVE, FAIRHOPE,  ALABAMA" "FAIRHOPE HIGH SCHOOL, 1 PIRATE DRIVE, FAIRHOPE,  ALABAMA" "FAIRHOPE HIGH SCHOOL, 1 PIRATE DRIVE, FAIRHOPE,  ALABAMA" "FAIRHOPE HIGH SCHOOL, 1 PIRATE DRIVE, FAIRHOPE,  ALABAMA" ...
##  $ State Name                   : chr  "Alabama" "Alabama" "Alabama" "Alabama" ...
##  $ County Name                  : chr  "Baldwin" "Baldwin" "Baldwin" "Baldwin" ...
##  $ City Name                    : chr  "Fairhope" "Fairhope" "Fairhope" "Fairhope" ...
##  $ CBSA Name                    : chr  "Daphne-Fairhope-Foley, AL" "Daphne-Fairhope-Foley, AL" "Daphne-Fairhope-Foley, AL" "Daphne-Fairhope-Foley, AL" ...
##  $ Date of Last Change          : Date, format: "2018-07-18" "2018-07-18" ...
##  - attr(*, "spec")=List of 2
##   ..$ cols   :List of 55
##   .. ..$ State Code                   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ County Code                  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Site Num                     : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Parameter Code               : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ POC                          : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Latitude                     : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ Longitude                    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ Datum                        : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Parameter Name               : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Sample Duration              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Pollutant Standard           : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Metric Used                  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Method Name                  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Year                         : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Units of Measure             : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Event Type                   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Observation Count            : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Observation Percent          : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Completeness Indicator       : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Valid Day Count              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Required Day Count           : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Exceptional Data Count       : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Null Data Count              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Primary Exceedance Count     : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Secondary Exceedance Count   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Certification Indicator      : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Num Obs Below MDL            : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ Arithmetic Mean              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ Arithmetic Standard Dev      : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 1st Max Value                : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 1st Max DateTime             :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 2nd Max Value                : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 2nd Max DateTime             :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 3rd Max Value                : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 3rd Max DateTime             :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 4th Max Value                : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 4th Max DateTime             :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 1st Max Non Overlapping Value: list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 1st NO Max DateTime          :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 2nd Max Non Overlapping Value: list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 2nd NO Max DateTime          :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_datetime" "collector"
##   .. ..$ 99th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 98th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 95th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 90th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 75th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 50th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ 10th Percentile              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
##   .. ..$ Local Site Name              : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Address                      : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ State Name                   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ County Name                  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ City Name                    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ CBSA Name                    : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Date of Last Change          :List of 1
##   .. .. ..$ format: chr ""
##   .. .. ..- attr(*, "class")= chr  "collector_date" "collector"
##   ..$ default: list()
##   .. ..- attr(*, "class")= chr  "collector_guess" "collector"
##   ..- attr(*, "class")= chr "col_spec"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(ozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   State Code        County Code          Site Num         Parameter Code 
##  Length:66869       Length:66869       Length:66869       Min.   :11101  
##  Class :character   Class :character   Class :character   1st Qu.:43828  
##  Mode  :character   Mode  :character   Mode  :character   Median :68101  
##                                                           Mean   :64847  
##                                                           3rd Qu.:88110  
##                                                           Max.   :88503  
##                                                                          
##       POC            Latitude       Longitude          Datum          
##  Min.   : 1.000   Min.   :17.71   Min.   :-160.51   Length:66869      
##  1st Qu.: 1.000   1st Qu.:34.88   1st Qu.:-111.88   Class :character  
##  Median : 1.000   Median :39.31   Median : -91.18   Mode  :character  
##  Mean   : 2.813   Mean   :38.60   Mean   : -95.85                     
##  3rd Qu.: 5.000   3rd Qu.:41.89   3rd Qu.: -81.34                     
##  Max.   :99.000   Max.   :64.85   Max.   : -64.78                     
##                                                                       
##  Parameter Name     Sample Duration    Pollutant Standard
##  Length:66869       Length:66869       Length:66869      
##  Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character  
##                                                          
##                                                          
##                                                          
##                                                          
##  Metric Used        Method Name             Year      Units of Measure  
##  Length:66869       Length:66869       Min.   :2017   Length:66869      
##  Class :character   Class :character   1st Qu.:2017   Class :character  
##  Mode  :character   Mode  :character   Median :2017   Mode  :character  
##                                        Mean   :2017                     
##                                        3rd Qu.:2017                     
##                                        Max.   :2017                     
##                                                                         
##   Event Type        Observation Count Observation Percent
##  Length:66869       Min.   :     1    Min.   :  0.00     
##  Class :character   1st Qu.:    59    1st Qu.: 89.00     
##  Mode  :character   Median :   116    Median : 96.00     
##                     Mean   :  2482    Mean   : 86.59     
##                     3rd Qu.:  4422    3rd Qu.: 98.00     
##                     Max.   :104683    Max.   :100.00     
##                                                          
##  Completeness Indicator Valid Day Count Required Day Count
##  Length:66869           Min.   :  0.0   Min.   :  0.0     
##  Class :character       1st Qu.: 58.0   1st Qu.: 61.0     
##  Mode  :character       Median :113.0   Median :122.0     
##                         Mean   :155.8   Mean   :177.9     
##                         3rd Qu.:313.0   3rd Qu.:365.0     
##                         Max.   :365.0   Max.   :365.0     
##                                                           
##  Exceptional Data Count Null Data Count   Primary Exceedance Count
##  Min.   :     0.00      Min.   :    0.0   Min.   :  0.00          
##  1st Qu.:     0.00      1st Qu.:    0.0   1st Qu.:  0.00          
##  Median :     0.00      Median :    3.0   Median :  0.00          
##  Mean   :    57.87      Mean   :  116.8   Mean   :  1.49          
##  3rd Qu.:     0.00      3rd Qu.:   13.0   3rd Qu.:  0.00          
##  Max.   :102125.00      Max.   :40219.0   Max.   :230.00          
##                                           NA's   :54622           
##  Secondary Exceedance Count Certification Indicator Num Obs Below MDL
##  Min.   :  0.00             Length:66869            Min.   :0        
##  1st Qu.:  0.00             Class :character        1st Qu.:0        
##  Median :  0.00             Mode  :character        Median :0        
##  Mean   :  1.47                                     Mean   :0        
##  3rd Qu.:  0.00                                     3rd Qu.:0        
##  Max.   :114.00                                     Max.   :0        
##  NA's   :55603                                                       
##  Arithmetic Mean    Arithmetic Standard Dev 1st Max Value     
##  Min.   :  -14.63   Min.   :    0.00        Min.   :    -1.5  
##  1st Qu.:    0.04   1st Qu.:    0.01        1st Qu.:     0.1  
##  Median :    0.40   Median :    0.30        Median :     2.0  
##  Mean   :   43.99   Mean   :    8.77        Mean   :   111.0  
##  3rd Qu.:    7.72   3rd Qu.:    4.12        3rd Qu.:    29.1  
##  Max.   :50128.95   Max.   :34512.06        Max.   :336583.0  
##                                                               
##  1st Max DateTime              2nd Max Value      
##  Min.   :2017-01-01 00:00:00   Min.   :    -2.90  
##  1st Qu.:2017-02-24 00:00:00   1st Qu.:     0.07  
##  Median :2017-06-20 10:00:00   Median :     1.51  
##  Mean   :2017-06-17 15:18:29   Mean   :    98.68  
##  3rd Qu.:2017-09-16 00:00:00   3rd Qu.:    25.09  
##  Max.   :2017-12-31 23:00:00   Max.   :304500.00  
##                                NA's   :243        
##  2nd Max DateTime              3rd Max Value      
##  Min.   :2017-01-01 00:00:00   Min.   :    -9.10  
##  1st Qu.:2017-02-23 14:00:00   1st Qu.:     0.07  
##  Median :2017-06-18 00:00:00   Median :     1.34  
##  Mean   :2017-06-15 21:14:51   Mean   :    93.74  
##  3rd Qu.:2017-09-14 00:00:00   3rd Qu.:    23.39  
##  Max.   :2017-12-31 23:00:00   Max.   :299667.00  
##  NA's   :243                   NA's   :291        
##  3rd Max DateTime              4th Max Value      
##  Min.   :2017-01-01 00:00:00   Min.   :    -8.20  
##  1st Qu.:2017-02-22 10:15:00   1st Qu.:     0.07  
##  Median :2017-06-18 00:00:00   Median :     1.22  
##  Mean   :2017-06-15 13:34:28   Mean   :    90.12  
##  3rd Qu.:2017-09-13 10:00:00   3rd Qu.:    22.20  
##  Max.   :2017-12-31 23:00:00   Max.   :294333.00  
##  NA's   :291                   NA's   :651        
##  4th Max DateTime              1st Max Non Overlapping Value
##  Min.   :2017-01-01 00:00:00   Min.   :0.10                 
##  1st Qu.:2017-02-24 00:00:00   1st Qu.:0.80                 
##  Median :2017-06-19 15:00:00   Median :1.20                 
##  Mean   :2017-06-16 18:56:28   Mean   :1.38                 
##  3rd Qu.:2017-09-14 10:00:00   3rd Qu.:1.70                 
##  Max.   :2017-12-31 23:00:00   Max.   :7.50                 
##  NA's   :651                   NA's   :66564                
##  1st NO Max DateTime           2nd Max Non Overlapping Value
##  Min.   :2017-01-01 05:00:00   Min.   :0.10                 
##  1st Qu.:2017-02-08 02:00:00   1st Qu.:0.70                 
##  Median :2017-11-10 08:00:00   Median :1.10                 
##  Mean   :2017-08-01 02:00:23   Mean   :1.19                 
##  3rd Qu.:2017-12-09 22:00:00   3rd Qu.:1.50                 
##  Max.   :2017-12-31 15:00:00   Max.   :4.50                 
##  NA's   :66564                 NA's   :66564                
##  2nd NO Max DateTime           99th Percentile     98th Percentile    
##  Min.   :2017-01-01 05:00:00   Min.   :    -1.50   Min.   :    -1.50  
##  1st Qu.:2017-02-09 23:00:00   1st Qu.:     0.08   1st Qu.:     0.07  
##  Median :2017-10-21 00:00:00   Median :     1.52   Median :     1.30  
##  Mean   :2017-08-09 04:12:35   Mean   :    70.87   Mean   :    65.12  
##  3rd Qu.:2017-12-11 23:00:00   3rd Qu.:    22.90   3rd Qu.:    19.80  
##  Max.   :2017-12-31 23:00:00   Max.   :175250.00   Max.   :147467.00  
##  NA's   :66564                                                        
##  95th Percentile     90th Percentile    75th Percentile   
##  Min.   :    -1.50   Min.   :   -1.50   Min.   :   -2.90  
##  1st Qu.:     0.06   1st Qu.:    0.05   1st Qu.:    0.04  
##  Median :     1.00   Median :    0.80   Median :    0.50  
##  Mean   :    58.71   Mean   :   53.99   Mean   :   47.99  
##  3rd Qu.:    16.00   3rd Qu.:   13.20   3rd Qu.:    9.70  
##  Max.   :118383.00   Max.   :94867.00   Max.   :61858.00  
##                                                           
##  50th Percentile    10th Percentile     Local Site Name   
##  Min.   :  -19.00   Min.   :  -40.800   Length:66869      
##  1st Qu.:    0.01   1st Qu.:    0.000   Class :character  
##  Median :    0.29   Median :    0.079   Mode  :character  
##  Mean   :   42.79   Mean   :   35.142                     
##  3rd Qu.:    6.60   3rd Qu.:    2.500                     
##  Max.   :40875.00   Max.   :18725.000                     
##                                                           
##    Address           State Name        County Name       
##  Length:66869       Length:66869       Length:66869      
##  Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character  
##                                                          
##                                                          
##                                                          
##                                                          
##   City Name          CBSA Name         Date of Last Change 
##  Length:66869       Length:66869       Min.   :2017-02-03  
##  Class :character   Class :character   1st Qu.:2018-02-26  
##  Mode  :character   Mode  :character   Median :2018-04-13  
##                                        Mean   :2018-05-12  
##                                        3rd Qu.:2018-08-02  
##                                        Max.   :2018-11-27  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(ozone, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 55
##    `State Code` `County Code` `Site Num` `Parameter Code`   POC Latitude
##    <chr>        <chr>         <chr>                 <int> <int>    <dbl>
##  1 01           003           0010                  44201     1     30.5
##  2 01           003           0010                  44201     1     30.5
##  3 01           003           0010                  44201     1     30.5
##  4 01           003           0010                  44201     1     30.5
##  5 01           003           0010                  88101     1     30.5
##  6 01           003           0010                  88101     1     30.5
##  7 01           003           0010                  88101     1     30.5
##  8 01           003           0010                  88101     1     30.5
##  9 01           027           0001                  88101     1     33.3
## 10 01           027           0001                  88101     1     33.3
## # ... with 49 more variables: Longitude <dbl>, Datum <chr>, `Parameter
## #   Name` <chr>, `Sample Duration` <chr>, `Pollutant Standard` <chr>,
## #   `Metric Used` <chr>, `Method Name` <chr>, Year <int>, `Units of
## #   Measure` <chr>, `Event Type` <chr>, `Observation Count` <int>,
## #   `Observation Percent` <int>, `Completeness Indicator` <chr>, `Valid
## #   Day Count` <int>, `Required Day Count` <int>, `Exceptional Data
## #   Count` <int>, `Null Data Count` <int>, `Primary Exceedance
## #   Count` <int>, `Secondary Exceedance Count` <int>, `Certification
## #   Indicator` <chr>, `Num Obs Below MDL` <int>, `Arithmetic Mean` <dbl>,
## #   `Arithmetic Standard Dev` <dbl>, `1st Max Value` <dbl>, `1st Max
## #   DateTime` <dttm>, `2nd Max Value` <dbl>, `2nd Max DateTime` <dttm>,
## #   `3rd Max Value` <dbl>, `3rd Max DateTime` <dttm>, `4th Max
## #   Value` <dbl>, `4th Max DateTime` <dttm>, `1st Max Non Overlapping
## #   Value` <dbl>, `1st NO Max DateTime` <dttm>, `2nd Max Non Overlapping
## #   Value` <dbl>, `2nd NO Max DateTime` <dttm>, `99th Percentile` <dbl>,
## #   `98th Percentile` <dbl>, `95th Percentile` <dbl>, `90th
## #   Percentile` <dbl>, `75th Percentile` <dbl>, `50th Percentile` <dbl>,
## #   `10th Percentile` <dbl>, `Local Site Name` <chr>, Address <chr>,
## #   `State Name` <chr>, `County Name` <chr>, `City Name` <chr>, `CBSA
## #   Name` <chr>, `Date of Last Change` <date>
\end{verbatim}

\paragraph{Importing, saving and managing
data}\label{importing-saving-and-managing-data}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'tidyverse' was built under R version 3.5.3
\end{verbatim}

\begin{verbatim}
## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.0.0     v purrr   0.2.5
## v tibble  1.4.2     v stringr 1.3.1
## v tidyr   0.8.2     v forcats 0.3.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() --
## x tibble::as_data_frame() masks dplyr::as_data_frame(), igraph::as_data_frame()
## x purrr::compose()        masks igraph::compose()
## x tidyr::crossing()       masks igraph::crossing()
## x dplyr::filter()         masks stats::filter()
## x dplyr::groups()         masks igraph::groups()
## x dplyr::lag()            masks stats::lag()
## x purrr::simplify()       masks igraph::simplify()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(ozone) <-}\StringTok{ }\KeywordTok{make.names}\NormalTok{(}\KeywordTok{names}\NormalTok{(ozone))}
\KeywordTok{select}\NormalTok{(ozone, State.Name) }\OperatorTok{%>%}\StringTok{ }\NormalTok{unique }\OperatorTok{%>%}\StringTok{ }\NormalTok{nrow}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 54
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unique}\NormalTok{(ozone}\OperatorTok{$}\NormalTok{State.Name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Alabama"              "Alaska"               "Arizona"             
##  [4] "Arkansas"             "California"           "Colorado"            
##  [7] "Connecticut"          "Delaware"             "District Of Columbia"
## [10] "Florida"              "Georgia"              "Hawaii"              
## [13] "Idaho"                "Illinois"             "Indiana"             
## [16] "Iowa"                 "Kansas"               "Kentucky"            
## [19] "Louisiana"            "Maine"                "Maryland"            
## [22] "Massachusetts"        "Michigan"             "Minnesota"           
## [25] "Mississippi"          "Missouri"             "Montana"             
## [28] "Nebraska"             "Nevada"               "New Hampshire"       
## [31] "New Jersey"           "New Mexico"           "New York"            
## [34] "North Carolina"       "North Dakota"         "Ohio"                
## [37] "Oklahoma"             "Oregon"               "Pennsylvania"        
## [40] "Rhode Island"         "South Carolina"       "South Dakota"        
## [43] "Tennessee"            "Texas"                "Utah"                
## [46] "Vermont"              "Virginia"             "Washington"          
## [49] "West Virginia"        "Wisconsin"            "Wyoming"             
## [52] "Puerto Rico"          "Virgin Islands"       "Country Of Mexico"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{70}\NormalTok{,}\DecValTok{80}\NormalTok{,}\DecValTok{95}\NormalTok{)}
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20 95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15 20 50 70 95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(starwars, species }\OperatorTok{==}\StringTok{ "Human"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 35 x 13
##    name  height  mass hair_color skin_color eye_color birth_year gender
##    <chr>  <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> 
##  1 Luke~    172    77 blond      fair       blue            19   male  
##  2 Dart~    202   136 none       white      yellow          41.9 male  
##  3 Leia~    150    49 brown      light      brown           19   female
##  4 Owen~    178   120 brown, gr~ light      blue            52   male  
##  5 Beru~    165    75 brown      light      blue            47   female
##  6 Bigg~    183    84 black      light      brown           24   male  
##  7 Obi-~    182    77 auburn, w~ fair       blue-gray       57   male  
##  8 Anak~    188    84 blond      fair       blue            41.9 male  
##  9 Wilh~    180    NA auburn, g~ fair       blue            64   male  
## 10 Han ~    180    80 brown      fair       brown           29   male  
## # ... with 25 more rows, and 5 more variables: homeworld <chr>,
## #   species <chr>, films <list>, vehicles <list>, starships <list>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{income <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"~/A-HU CPT/ANLY 506/income.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   Occupation = col_character(),
##   Industry = col_character(),
##   All_workers = col_integer(),
##   All_weekly = col_integer(),
##   M_workers = col_integer(),
##   M_weekly = col_integer(),
##   F_workers = col_integer(),
##   F_weekly = col_integer()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly,}\DataTypeTok{trim =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 973.9112
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly,}\DataTypeTok{trim =} \DecValTok{0}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 924
\end{verbatim}

\paragraph{Practical stats}\label{practical-stats}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(matrixStats)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'matrixStats' was built under R version 3.5.3
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'matrixStats'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     count
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{weighted.mean}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly,}\DataTypeTok{w=}\NormalTok{income}\OperatorTok{$}\NormalTok{Industry,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in weighted.mean.default(income$M_weekly, w = income$Industry,
## na.rm = TRUE): NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weighted_median <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(M_weekly, Industry,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{\{}
  
  \ControlFlowTok{if}\NormalTok{(na.rm)\{}
    
\NormalTok{    df_omit <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(M_weekly, Industry))}
    
    \KeywordTok{return}\NormalTok{(}\KeywordTok{weightedMedian}\NormalTok{(df_omit}\OperatorTok{$}\NormalTok{M_weekly, df_omit}\OperatorTok{$}\NormalTok{Industry,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
    
\NormalTok{  \} }
  
  \KeywordTok{weightedMedian}\NormalTok{(M_weekly, Industry,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
  
\NormalTok{\}}
\KeywordTok{weighted_median}\NormalTok{(income}\OperatorTok{$}\NormalTok{M_weekly,income}\OperatorTok{$}\NormalTok{Industry,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 883
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(income}\OperatorTok{$}\NormalTok{F_workers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 227.3746
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mad}\NormalTok{(income}\OperatorTok{$}\NormalTok{F_workers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23.7216
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{iqr}\NormalTok{(income}\OperatorTok{$}\NormalTok{F_workers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 66
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(ozone) <-}\StringTok{ }\KeywordTok{make.names}\NormalTok{(}\KeywordTok{names}\NormalTok{(ozone))}
\KeywordTok{select}\NormalTok{(ozone, State.Name) }\OperatorTok{%>%}\StringTok{ }\NormalTok{unique }\OperatorTok{%>%}\StringTok{ }\NormalTok{nrow}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 54
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unique}\NormalTok{(ozone}\OperatorTok{$}\NormalTok{State.Name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Alabama"              "Alaska"               "Arizona"             
##  [4] "Arkansas"             "California"           "Colorado"            
##  [7] "Connecticut"          "Delaware"             "District Of Columbia"
## [10] "Florida"              "Georgia"              "Hawaii"              
## [13] "Idaho"                "Illinois"             "Indiana"             
## [16] "Iowa"                 "Kansas"               "Kentucky"            
## [19] "Louisiana"            "Maine"                "Maryland"            
## [22] "Massachusetts"        "Michigan"             "Minnesota"           
## [25] "Mississippi"          "Missouri"             "Montana"             
## [28] "Nebraska"             "Nevada"               "New Hampshire"       
## [31] "New Jersey"           "New Mexico"           "New York"            
## [34] "North Carolina"       "North Dakota"         "Ohio"                
## [37] "Oklahoma"             "Oregon"               "Pennsylvania"        
## [40] "Rhode Island"         "South Carolina"       "South Dakota"        
## [43] "Tennessee"            "Texas"                "Utah"                
## [46] "Vermont"              "Virginia"             "Washington"          
## [49] "West Virginia"        "Wisconsin"            "Wyoming"             
## [52] "Puerto Rico"          "Virgin Islands"       "Country Of Mexico"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(mtcars[[}\StringTok{"mpg"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.026948
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{IQR}\NormalTok{(mtcars[[}\StringTok{"mpg"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.375
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mad}\NormalTok{(mtcars[[}\StringTok{"mpg"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.41149
\end{verbatim}

\paragraph{Data transformation}\label{data-transformation}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tibble)}
\NormalTok{income <-}\StringTok{ }\KeywordTok{as.tibble}\NormalTok{(income)}
\NormalTok{income1 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(income, M_weekly }\OperatorTok{>}\StringTok{ }\DecValTok{2000}\NormalTok{)}
\KeywordTok{str}\NormalTok{(income1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    2 obs. of  8 variables:
##  $ Occupation : chr  "Chief executives" "Pharmacists"
##  $ Industry   : chr  "Management" "Healthcare Professional"
##  $ All_workers: int  1046 206
##  $ All_weekly : int  2041 1920
##  $ M_workers  : int  763 98
##  $ M_weekly   : int  2251 2117
##  $ F_workers  : int  283 108
##  $ F_weekly   : int  1836 1811
##  - attr(*, "spec")=List of 2
##   ..$ cols   :List of 8
##   .. ..$ Occupation : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ Industry   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
##   .. ..$ All_workers: list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ All_weekly : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ M_workers  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ M_weekly   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ F_workers  : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   .. ..$ F_weekly   : list()
##   .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
##   ..$ default: list()
##   .. ..- attr(*, "class")= chr  "collector_guess" "collector"
##   ..- attr(*, "class")= chr "col_spec"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(income1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 8
##   Occupation Industry All_workers All_weekly M_workers M_weekly F_workers
##   <chr>      <chr>          <int>      <int>     <int>    <int>     <int>
## 1 Chief exe~ Managem~        1046       2041       763     2251       283
## 2 Pharmacis~ Healthc~         206       1920        98     2117       108
## # ... with 1 more variable: F_weekly <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nycflights13)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'nycflights13' was built under R version 3.5.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{'flights'}\NormalTok{)}
\KeywordTok{str}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    336776 obs. of  19 variables:
##  $ year          : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
##  $ month         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ day           : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ dep_time      : int  517 533 542 544 554 554 555 557 557 558 ...
##  $ sched_dep_time: int  515 529 540 545 600 558 600 600 600 600 ...
##  $ dep_delay     : num  2 4 2 -1 -6 -4 -5 -3 -3 -2 ...
##  $ arr_time      : int  830 850 923 1004 812 740 913 709 838 753 ...
##  $ sched_arr_time: int  819 830 850 1022 837 728 854 723 846 745 ...
##  $ arr_delay     : num  11 20 33 -18 -25 12 19 -14 -8 8 ...
##  $ carrier       : chr  "UA" "UA" "AA" "B6" ...
##  $ flight        : int  1545 1714 1141 725 461 1696 507 5708 79 301 ...
##  $ tailnum       : chr  "N14228" "N24211" "N619AA" "N804JB" ...
##  $ origin        : chr  "EWR" "LGA" "JFK" "JFK" ...
##  $ dest          : chr  "IAH" "IAH" "MIA" "BQN" ...
##  $ air_time      : num  227 227 160 183 116 150 158 53 140 138 ...
##  $ distance      : num  1400 1416 1089 1576 762 ...
##  $ hour          : num  5 5 5 5 6 5 6 6 6 6 ...
##  $ minute        : num  15 29 40 45 0 58 0 0 0 0 ...
##  $ time_hour     : POSIXct, format: "2013-01-01 05:00:00" "2013-01-01 05:00:00" ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights1 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, day }\OperatorTok{==}\StringTok{ "30"}\NormalTok{, month }\OperatorTok{==}\StringTok{ "12"}\NormalTok{)}
\NormalTok{flights2 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights1, dep_delay }\OperatorTok{==}\StringTok{ "NA"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(flights2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       year         month          day         dep_time   sched_dep_time
##  Min.   : NA   Min.   : NA   Min.   : NA   Min.   : NA   Min.   : NA   
##  1st Qu.: NA   1st Qu.: NA   1st Qu.: NA   1st Qu.: NA   1st Qu.: NA   
##  Median : NA   Median : NA   Median : NA   Median : NA   Median : NA   
##  Mean   :NaN   Mean   :NaN   Mean   :NaN   Mean   :NaN   Mean   :NaN   
##  3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA   
##  Max.   : NA   Max.   : NA   Max.   : NA   Max.   : NA   Max.   : NA   
##    dep_delay      arr_time   sched_arr_time   arr_delay  
##  Min.   : NA   Min.   : NA   Min.   : NA    Min.   : NA  
##  1st Qu.: NA   1st Qu.: NA   1st Qu.: NA    1st Qu.: NA  
##  Median : NA   Median : NA   Median : NA    Median : NA  
##  Mean   :NaN   Mean   :NaN   Mean   :NaN    Mean   :NaN  
##  3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA    3rd Qu.: NA  
##  Max.   : NA   Max.   : NA   Max.   : NA    Max.   : NA  
##    carrier              flight      tailnum             origin         
##  Length:0           Min.   : NA   Length:0           Length:0          
##  Class :character   1st Qu.: NA   Class :character   Class :character  
##  Mode  :character   Median : NA   Mode  :character   Mode  :character  
##                     Mean   :NaN                                        
##                     3rd Qu.: NA                                        
##                     Max.   : NA                                        
##      dest              air_time      distance        hour    
##  Length:0           Min.   : NA   Min.   : NA   Min.   : NA  
##  Class :character   1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
##  Mode  :character   Median : NA   Median : NA   Median : NA  
##                     Mean   :NaN   Mean   :NaN   Mean   :NaN  
##                     3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
##                     Max.   : NA   Max.   : NA   Max.   : NA  
##      minute      time_hour 
##  Min.   : NA   Min.   :NA  
##  1st Qu.: NA   1st Qu.:NA  
##  Median : NA   Median :NA  
##  Mean   :NaN   Mean   :NA  
##  3rd Qu.: NA   3rd Qu.:NA  
##  Max.   : NA   Max.   :NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(flights2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 0 x 19
## # ... with 19 variables: year <int>, month <int>, day <int>,
## #   dep_time <int>, sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,
## #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,
## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,
## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights3 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(flights, dep_delay }\OperatorTok{==}\StringTok{ "NULL"}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(flights1}\OperatorTok{$}\NormalTok{dep_delay,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.69811
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(flights2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 0 x 19
## # ... with 19 variables: year <int>, month <int>, day <int>,
## #   dep_time <int>, sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,
## #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>, flight <int>,
## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,
## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income1 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(income,}\DataTypeTok{diff =}\NormalTok{ M_weekly }\OperatorTok{-}\StringTok{ }\NormalTok{F_weekly)}
\NormalTok{income1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 535
## Variables: 9
## $ Occupation  <chr> "Chief executives", "General and operations manage...
## $ Industry    <chr> "Management", "Management", "Management", "Managem...
## $ All_workers <int> 1046, 823, 8, 55, 948, 59, 170, 636, 1124, 23, 254...
## $ All_weekly  <int> 2041, 1260, NA, 1050, 1462, 1557, 1191, 1728, 1408...
## $ M_workers   <int> 763, 621, 5, 29, 570, 24, 96, 466, 551, 7, 68, 17,...
## $ M_weekly    <int> 2251, 1347, NA, NA, 1603, NA, 1451, 1817, 1732, NA...
## $ F_workers   <int> 283, 202, 4, 26, 378, 35, 73, 169, 573, 16, 186, 2...
## $ F_weekly    <int> 1836, 1002, NA, NA, 1258, NA, 981, 1563, 1130, NA,...
## $ diff        <int> 415, 345, NA, NA, 345, NA, 470, 254, 602, NA, 221,...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(income1}\OperatorTok{$}\NormalTok{diff,}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{round}\NormalTok{(sd, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 149.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{data}\NormalTok{(}\StringTok{"iris"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris2<-iris }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{filter}\NormalTok{(Petal.Length }\OperatorTok{>}\StringTok{ }\FloatTok{4.5}\NormalTok{)}
\NormalTok{iris3<-iris2 }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{filter}\NormalTok{(Species}\OperatorTok{==}\StringTok{"virginica"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(iris3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :5.600   Min.   :2.200   Min.   :4.800   Min.   :1.400  
##  1st Qu.:6.300   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  
##  Median :6.500   Median :3.000   Median :5.600   Median :2.000  
##  Mean   :6.622   Mean   :2.984   Mean   :5.573   Mean   :2.033  
##  3rd Qu.:6.900   3rd Qu.:3.200   3rd Qu.:5.900   3rd Qu.:2.300  
##  Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    : 0  
##  versicolor: 0  
##  virginica :49  
##                 
##                 
## 
\end{verbatim}

\paragraph{Data visualization}\label{data-visualization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"faithful"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(faithful)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    eruptions        waiting    
##  Min.   :1.600   Min.   :43.0  
##  1st Qu.:2.163   1st Qu.:58.0  
##  Median :4.000   Median :76.0  
##  Mean   :3.488   Mean   :70.9  
##  3rd Qu.:4.454   3rd Qu.:82.0  
##  Max.   :5.100   Max.   :96.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(who)}
\KeywordTok{str}\NormalTok{(who)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    7240 obs. of  60 variables:
##  $ country     : chr  "Afghanistan" "Afghanistan" "Afghanistan" "Afghanistan" ...
##  $ iso2        : chr  "AF" "AF" "AF" "AF" ...
##  $ iso3        : chr  "AFG" "AFG" "AFG" "AFG" ...
##  $ year        : int  1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 ...
##  $ new_sp_m014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_m65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sp_f65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_m65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_sn_f65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_m65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ new_ep_f65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_m65  : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f014 : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f1524: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f2534: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f3544: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f4554: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f5564: int  NA NA NA NA NA NA NA NA NA NA ...
##  $ newrel_f65  : int  NA NA NA NA NA NA NA NA NA NA ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(who)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    country              iso2               iso3                year     
##  Length:7240        Length:7240        Length:7240        Min.   :1980  
##  Class :character   Class :character   Class :character   1st Qu.:1988  
##  Mode  :character   Mode  :character   Mode  :character   Median :1997  
##                                                           Mean   :1997  
##                                                           3rd Qu.:2005  
##                                                           Max.   :2013  
##                                                                         
##   new_sp_m014       new_sp_m1524    new_sp_m2534      new_sp_m3544    
##  Min.   :   0.00   Min.   :    0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:   0.00   1st Qu.:    9   1st Qu.:   14.0   1st Qu.:   13.0  
##  Median :   5.00   Median :   90   Median :  150.0   Median :  130.0  
##  Mean   :  83.71   Mean   : 1016   Mean   : 1403.8   Mean   : 1315.9  
##  3rd Qu.:  37.00   3rd Qu.:  502   3rd Qu.:  715.5   3rd Qu.:  583.5  
##  Max.   :5001.00   Max.   :78278   Max.   :84003.0   Max.   :90830.0  
##  NA's   :4067      NA's   :4031    NA's   :4034      NA's   :4021     
##   new_sp_m4554    new_sp_m5564       new_sp_m65       new_sp_f014     
##  Min.   :    0   Min.   :    0.0   Min.   :    0.0   Min.   :   0.00  
##  1st Qu.:   12   1st Qu.:    8.0   1st Qu.:    8.0   1st Qu.:   1.00  
##  Median :  102   Median :   63.0   Median :   53.0   Median :   7.00  
##  Mean   : 1104   Mean   :  800.7   Mean   :  682.8   Mean   : 114.33  
##  3rd Qu.:  440   3rd Qu.:  279.0   3rd Qu.:  232.0   3rd Qu.:  50.75  
##  Max.   :82921   Max.   :63814.0   Max.   :70376.0   Max.   :8576.00  
##  NA's   :4017    NA's   :4022      NA's   :4031      NA's   :4066     
##   new_sp_f1524      new_sp_f2534      new_sp_f3544      new_sp_f4554    
##  Min.   :    0.0   Min.   :    0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:    7.0   1st Qu.:    9.0   1st Qu.:    6.0   1st Qu.:    4.0  
##  Median :   66.0   Median :   84.0   Median :   57.0   Median :   38.0  
##  Mean   :  826.1   Mean   :  917.3   Mean   :  640.4   Mean   :  445.8  
##  3rd Qu.:  421.0   3rd Qu.:  476.2   3rd Qu.:  308.0   3rd Qu.:  211.0  
##  Max.   :53975.0   Max.   :49887.0   Max.   :34698.0   Max.   :23977.0  
##  NA's   :4046      NA's   :4040      NA's   :4041      NA's   :4036     
##   new_sp_f5564       new_sp_f65       new_sn_m014       new_sn_m1524    
##  Min.   :    0.0   Min.   :    0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:    3.0   1st Qu.:    4.0   1st Qu.:    1.0   1st Qu.:    2.0  
##  Median :   25.0   Median :   30.0   Median :    9.0   Median :   15.5  
##  Mean   :  313.9   Mean   :  283.9   Mean   :  308.7   Mean   :  513.0  
##  3rd Qu.:  146.5   3rd Qu.:  129.0   3rd Qu.:   61.0   3rd Qu.:  102.0  
##  Max.   :18203.0   Max.   :21339.0   Max.   :22355.0   Max.   :60246.0  
##  NA's   :4045      NA's   :4043      NA's   :6195      NA's   :6210     
##   new_sn_m2534      new_sn_m3544       new_sn_m4554      new_sn_m5564    
##  Min.   :    0.0   Min.   :     0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:    2.0   1st Qu.:     2.0   1st Qu.:    2.0   1st Qu.:    2.0  
##  Median :   23.0   Median :    19.0   Median :   19.0   Median :   16.0  
##  Mean   :  653.7   Mean   :   837.9   Mean   :  520.8   Mean   :  448.6  
##  3rd Qu.:  135.5   3rd Qu.:   132.0   3rd Qu.:  127.5   3rd Qu.:  101.0  
##  Max.   :50282.0   Max.   :250051.0   Max.   :57181.0   Max.   :64972.0  
##  NA's   :6218      NA's   :6215       NA's   :6213      NA's   :6219     
##    new_sn_m65       new_sn_f014     new_sn_f1524      new_sn_f2534    
##  Min.   :    0.0   Min.   :    0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:    2.0   1st Qu.:    1   1st Qu.:    1.0   1st Qu.:    2.0  
##  Median :   20.5   Median :    8   Median :   12.0   Median :   18.0  
##  Mean   :  460.4   Mean   :  292   Mean   :  407.9   Mean   :  466.3  
##  3rd Qu.:  111.8   3rd Qu.:   58   3rd Qu.:   89.0   3rd Qu.:  103.2  
##  Max.   :74282.0   Max.   :21406   Max.   :35518.0   Max.   :28753.0  
##  NA's   :6220      NA's   :6200    NA's   :6218      NA's   :6224     
##   new_sn_f3544        new_sn_f4554       new_sn_f5564    
##  Min.   :     0.00   Min.   :    0.00   Min.   :    0.0  
##  1st Qu.:     1.00   1st Qu.:    1.00   1st Qu.:    1.0  
##  Median :    11.00   Median :   10.00   Median :    8.0  
##  Mean   :   506.59   Mean   :  271.16   Mean   :  213.4  
##  3rd Qu.:    82.25   3rd Qu.:   76.75   3rd Qu.:   56.0  
##  Max.   :148811.00   Max.   :23869.00   Max.   :26085.0  
##  NA's   :6220        NA's   :6222       NA's   :6223     
##    new_sn_f65       new_ep_m014      new_ep_m1524     new_ep_m2534    
##  Min.   :    0.0   Min.   :   0.0   Min.   :   0.0   Min.   :    0.0  
##  1st Qu.:    1.0   1st Qu.:   0.0   1st Qu.:   1.0   1st Qu.:    1.0  
##  Median :   13.0   Median :   6.0   Median :  11.0   Median :   13.0  
##  Mean   :  230.8   Mean   : 128.6   Mean   : 158.3   Mean   :  201.2  
##  3rd Qu.:   74.0   3rd Qu.:  55.0   3rd Qu.:  88.0   3rd Qu.:  124.0  
##  Max.   :29630.0   Max.   :7869.0   Max.   :8558.0   Max.   :11843.0  
##  NA's   :6221      NA's   :6202     NA's   :6214     NA's   :6220     
##   new_ep_m3544        new_ep_m4554      new_ep_m5564       new_ep_m65     
##  Min.   :     0.00   Min.   :   0.00   Min.   :   0.00   Min.   :   0.00  
##  1st Qu.:     1.00   1st Qu.:   1.00   1st Qu.:   1.00   1st Qu.:   1.00  
##  Median :    10.50   Median :   8.50   Median :   7.00   Median :  10.00  
##  Mean   :   272.72   Mean   : 108.11   Mean   :  72.17   Mean   :  78.94  
##  3rd Qu.:    91.25   3rd Qu.:  63.25   3rd Qu.:  46.00   3rd Qu.:  55.00  
##  Max.   :105825.00   Max.   :5875.00   Max.   :3957.00   Max.   :3061.00  
##  NA's   :6216        NA's   :6220      NA's   :6225      NA's   :6222     
##   new_ep_f014      new_ep_f1524     new_ep_f2534      new_ep_f3544     
##  Min.   :   0.0   Min.   :   0.0   Min.   :    0.0   Min.   :     0.0  
##  1st Qu.:   0.0   1st Qu.:   1.0   1st Qu.:    1.0   1st Qu.:     1.0  
##  Median :   5.0   Median :   9.0   Median :   12.0   Median :     9.0  
##  Mean   : 112.9   Mean   : 149.2   Mean   :  189.5   Mean   :   241.7  
##  3rd Qu.:  50.0   3rd Qu.:  78.0   3rd Qu.:   95.0   3rd Qu.:    77.0  
##  Max.   :6960.0   Max.   :7866.0   Max.   :10759.0   Max.   :101015.0  
##  NA's   :6208     NA's   :6219     NA's   :6219      NA's   :6219      
##   new_ep_f4554      new_ep_f5564       new_ep_f65       newrel_m014     
##  Min.   :   0.00   Min.   :   0.00   Min.   :   0.00   Min.   :    0.0  
##  1st Qu.:   1.00   1st Qu.:   1.00   1st Qu.:   0.00   1st Qu.:    5.0  
##  Median :   8.00   Median :   6.00   Median :  10.00   Median :   32.5  
##  Mean   :  93.77   Mean   :  63.04   Mean   :  72.31   Mean   :  538.2  
##  3rd Qu.:  56.00   3rd Qu.:  42.00   3rd Qu.:  51.00   3rd Qu.:  210.0  
##  Max.   :6759.00   Max.   :4684.00   Max.   :2548.00   Max.   :18617.0  
##  NA's   :6223      NA's   :6223      NA's   :6226      NA's   :7050     
##   newrel_m1524      newrel_m2534    newrel_m3544       newrel_m4554     
##  Min.   :    0.0   Min.   :    0   Min.   :    0.00   Min.   :     0.0  
##  1st Qu.:   17.5   1st Qu.:   25   1st Qu.:   24.75   1st Qu.:    19.0  
##  Median :  171.0   Median :  217   Median :  208.00   Median :   175.0  
##  Mean   : 1489.5   Mean   : 2140   Mean   : 2036.40   Mean   :  1835.1  
##  3rd Qu.:  684.2   3rd Qu.: 1091   3rd Qu.:  851.25   3rd Qu.:   688.5  
##  Max.   :84785.0   Max.   :76917   Max.   :84565.00   Max.   :100297.0  
##  NA's   :7058      NA's   :7057    NA's   :7056       NA's   :7056      
##   newrel_m5564      newrel_m65        newrel_f014       newrel_f1524     
##  Min.   :     0   Min.   :     0.0   Min.   :    0.0   Min.   :    0.00  
##  1st Qu.:    13   1st Qu.:    17.0   1st Qu.:    5.0   1st Qu.:   10.75  
##  Median :   136   Median :   117.0   Median :   32.5   Median :  123.00  
##  Mean   :  1525   Mean   :  1426.0   Mean   :  532.8   Mean   : 1161.85  
##  3rd Qu.:   536   3rd Qu.:   453.5   3rd Qu.:  226.0   3rd Qu.:  587.75  
##  Max.   :112558   Max.   :124476.0   Max.   :18054.0   Max.   :49491.00  
##  NA's   :7055     NA's   :7058       NA's   :7050      NA's   :7056      
##   newrel_f2534      newrel_f3544      newrel_f4554      newrel_f5564    
##  Min.   :    0.0   Min.   :    0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:   18.0   1st Qu.:   12.5   1st Qu.:   10.0   1st Qu.:    8.0  
##  Median :  161.0   Median :  125.0   Median :   92.0   Median :   69.0  
##  Mean   : 1472.8   Mean   : 1125.0   Mean   :  877.3   Mean   :  686.4  
##  3rd Qu.:  762.5   3rd Qu.:  544.5   3rd Qu.:  400.5   3rd Qu.:  269.0  
##  Max.   :44985.0   Max.   :38804.0   Max.   :37138.0   Max.   :40892.0  
##  NA's   :7058      NA's   :7057      NA's   :7057      NA's   :7057     
##    newrel_f65     
##  Min.   :    0.0  
##  1st Qu.:    9.0  
##  Median :   69.0  
##  Mean   :  683.8  
##  3rd Qu.:  339.0  
##  Max.   :47438.0  
##  NA's   :7055
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(who)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 60
##   country iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534
##   <chr>   <chr> <chr> <int>       <int>        <int>        <int>
## 1 Afghan~ AF    AFG    1980          NA           NA           NA
## 2 Afghan~ AF    AFG    1981          NA           NA           NA
## 3 Afghan~ AF    AFG    1982          NA           NA           NA
## 4 Afghan~ AF    AFG    1983          NA           NA           NA
## 5 Afghan~ AF    AFG    1984          NA           NA           NA
## 6 Afghan~ AF    AFG    1985          NA           NA           NA
## # ... with 53 more variables: new_sp_m3544 <int>, new_sp_m4554 <int>,
## #   new_sp_m5564 <int>, new_sp_m65 <int>, new_sp_f014 <int>,
## #   new_sp_f1524 <int>, new_sp_f2534 <int>, new_sp_f3544 <int>,
## #   new_sp_f4554 <int>, new_sp_f5564 <int>, new_sp_f65 <int>,
## #   new_sn_m014 <int>, new_sn_m1524 <int>, new_sn_m2534 <int>,
## #   new_sn_m3544 <int>, new_sn_m4554 <int>, new_sn_m5564 <int>,
## #   new_sn_m65 <int>, new_sn_f014 <int>, new_sn_f1524 <int>,
## #   new_sn_f2534 <int>, new_sn_f3544 <int>, new_sn_f4554 <int>,
## #   new_sn_f5564 <int>, new_sn_f65 <int>, new_ep_m014 <int>,
## #   new_ep_m1524 <int>, new_ep_m2534 <int>, new_ep_m3544 <int>,
## #   new_ep_m4554 <int>, new_ep_m5564 <int>, new_ep_m65 <int>,
## #   new_ep_f014 <int>, new_ep_f1524 <int>, new_ep_f2534 <int>,
## #   new_ep_f3544 <int>, new_ep_f4554 <int>, new_ep_f5564 <int>,
## #   new_ep_f65 <int>, newrel_m014 <int>, newrel_m1524 <int>,
## #   newrel_m2534 <int>, newrel_m3544 <int>, newrel_m4554 <int>,
## #   newrel_m5564 <int>, newrel_m65 <int>, newrel_f014 <int>,
## #   newrel_f1524 <int>, newrel_f2534 <int>, newrel_f3544 <int>,
## #   newrel_f4554 <int>, newrel_f5564 <int>, newrel_f65 <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ mpg) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ displ, }\DataTypeTok{y =}\NormalTok{ hwy, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{, }\DataTypeTok{shape =} \StringTok{'24'}\NormalTok{,}\DataTypeTok{colour =} \StringTok{'black'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ANLY_506_-_Code_portfolio_files/figure-latex/unnamed-chunk-6-1.pdf}
\#\#\#\# Create Data frame

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Age <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{Name <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"James"}\NormalTok{, }\StringTok{"Mathew"}\NormalTok{, }\StringTok{"Olivia"}\NormalTok{, }\StringTok{"Stella"}\NormalTok{)}
\NormalTok{Gender <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"F"}\NormalTok{)}
\NormalTok{DataFrame =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{c}\NormalTok{(Age), }\KeywordTok{c}\NormalTok{(Name), }\KeywordTok{c}\NormalTok{(Gender))}
\KeywordTok{subset}\NormalTok{(DataFrame, Gender }\OperatorTok{==}\StringTok{ "M"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   c.Age. c.Name. c.Gender.
## 1     22   James         M
## 2     25  Mathew         M
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DataFrame =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Age,Name,Gender)}
\KeywordTok{subset}\NormalTok{(DataFrame,Gender}\OperatorTok{==}\StringTok{"M"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Age   Name Gender
## 1  22  James      M
## 2  25 Mathew      M
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(AirPassengers)}
\NormalTok{AirPassengers[AirPassengers }\OperatorTok{>=}\StringTok{ }\DecValTok{1949} \OperatorTok{&}\StringTok{ }\NormalTok{AirPassengers }\OperatorTok{<}\StringTok{ }\DecValTok{1950}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## numeric(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers[}\KeywordTok{time}\NormalTok{(AirPassengers) }\OperatorTok{>=}\StringTok{ }\DecValTok{1949} \OperatorTok{&}\StringTok{ }\KeywordTok{time}\NormalTok{(AirPassengers) }\OperatorTok{<}\StringTok{ }\DecValTok{1950}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 112 118 132 129 121 135 148 148 136 119 104 118
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{)}
\NormalTok{q <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(p, q)}
\NormalTok{x[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "A" "B" "C"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(w, v)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 2 7 8
## 
## [[2]]
## [1] "A" "B" "C"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{2}\NormalTok{]] <-}\StringTok{ "K"} 
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 2 7 8
## 
## [[2]]
## [1] "K"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{2}\NormalTok{]][}\DecValTok{1}\NormalTok{] <-}\StringTok{ "K"} 
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 2 7 8
## 
## [[2]]
## [1] "K"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{1}\NormalTok{]][}\DecValTok{2}\NormalTok{] <-}\StringTok{ "K"}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "2" "K" "8"
## 
## [[2]]
## [1] "K"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ }\KeywordTok{list}\NormalTok{ (}\StringTok{"x"}\NormalTok{=}\DecValTok{5}\NormalTok{, }\StringTok{"y"}\NormalTok{=}\DecValTok{10}\NormalTok{, }\StringTok{"z"}\NormalTok{=}\DecValTok{15}\NormalTok{)}

\NormalTok{y <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\NormalTok{data1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{id=}\KeywordTok{c}\NormalTok{(}\StringTok{"ID.1"}\NormalTok{, }\StringTok{"ID.2"}\NormalTok{, }\StringTok{"ID.3"}\NormalTok{))}
\NormalTok{sample1=}\KeywordTok{c}\NormalTok{(}\FloatTok{5.01}\NormalTok{, }\FloatTok{79.40}\NormalTok{, }\FloatTok{80.37}\NormalTok{)}
\NormalTok{sample2=}\KeywordTok{c}\NormalTok{(}\FloatTok{5.12}\NormalTok{, }\FloatTok{81.42}\NormalTok{, }\FloatTok{83.12}\NormalTok{)}
\NormalTok{sample3=}\KeywordTok{c}\NormalTok{(}\FloatTok{8.62}\NormalTok{, }\FloatTok{81.29}\NormalTok{, }\FloatTok{85.92}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\paragraph{Calculate distance}\label{calculate-distance}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}
\NormalTok{fa <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{fb <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{fm <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(fa,fb), }\DataTypeTok{byrow =}\NormalTok{ T, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{fm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    3    4    7    8
## [2,]    5    7    7    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(fm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1
## 2 4.123106
\end{verbatim}

\paragraph{Evaluate the best number of clusters using gap
method.}\label{evaluate-the-best-number-of-clusters-using-gap-method.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NbClust)}
          
\KeywordTok{library}\NormalTok{ (cluster)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'cluster' was built under R version 3.5.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{ (clustertend)}
          
\KeywordTok{library}\NormalTok{ (factoextra)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'factoextra' was built under R version 3.5.3
\end{verbatim}

\begin{verbatim}
## Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cluster1 <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"~/A-HU CPT/ANLY 506/Cluster 1.txt"}\NormalTok{, }\DataTypeTok{quote=}\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, }\DataTypeTok{comment.char=}\StringTok{""}\NormalTok{)}
\KeywordTok{View}\NormalTok{(Cluster1)}
\NormalTok{Cluster2 <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"~/A-HU CPT/ANLY 506/Cluster 2.txt"}\NormalTok{, }\DataTypeTok{quote=}\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, }\DataTypeTok{comment.char=}\StringTok{""}\NormalTok{)}
\KeywordTok{View}\NormalTok{(Cluster2)}
\NormalTok{Cluster3 <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"~/A-HU CPT/ANLY 506/Cluster 3.txt"}\NormalTok{, }\DataTypeTok{quote=}\StringTok{"}\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, }\DataTypeTok{comment.char=}\StringTok{""}\NormalTok{)}
\KeywordTok{View}\NormalTok{(Cluster3)}
\NormalTok{clusters <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Cluster1, Cluster2, Cluster3)}
\KeywordTok{names}\NormalTok{(clusters) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"c1"}\NormalTok{, }\StringTok{"c2"}\NormalTok{, }\StringTok{"c3"}\NormalTok{)}
\KeywordTok{View}\NormalTok{(clusters)}
\NormalTok{clusters <-}\StringTok{ }\NormalTok{clusters[, }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{)]}
\KeywordTok{fviz_nbclust}\NormalTok{(clusters, kmeans, }\DataTypeTok{method =} \StringTok{"gap_stat"}\NormalTok{)}\OperatorTok{+}\StringTok{ }\KeywordTok{theme_classic}\NormalTok{()     }
\end{Highlighting}
\end{Shaded}

\includegraphics{ANLY_506_-_Code_portfolio_files/figure-latex/unnamed-chunk-9-1.pdf}

\paragraph{Perform agglomerative
clustering}\label{perform-agglomerative-clustering}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{USArrests}
\NormalTok{df <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(df)}
\NormalTok{df <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(df)}
\KeywordTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Murder   Assault   UrbanPop         Rape
## Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
## Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
## Arizona    0.07163341 1.4788032  0.9989801  1.042878388
## Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
## California 0.27826823 1.2628144  1.7589234  2.067820292
## Colorado   0.02571456 0.3988593  0.8608085  1.864967207
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(df, }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\NormalTok{hc2 <-}\StringTok{ }\KeywordTok{agnes}\NormalTok{(df, }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{)}
\NormalTok{hc2}\OperatorTok{$}\NormalTok{ac}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8531583
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ac <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}\KeywordTok{agnes}\NormalTok{(df, }\DataTypeTok{method =}\NormalTok{ x)}\OperatorTok{$}\NormalTok{ac\}}
\NormalTok{m <-}\StringTok{ }\KeywordTok{c}\NormalTok{( }\StringTok{"average"}\NormalTok{, }\StringTok{"single"}\NormalTok{, }\StringTok{"complete"}\NormalTok{, }\StringTok{"ward"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(m) <-}\StringTok{ }\KeywordTok{c}\NormalTok{( }\StringTok{"average"}\NormalTok{, }\StringTok{"single"}\NormalTok{, }\StringTok{"complete"}\NormalTok{, }\StringTok{"ward"}\NormalTok{)}
\KeywordTok{map_dbl}\NormalTok{(m, ac)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210
\end{verbatim}

\subsection{III. Reference}\label{iii.-reference}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Art of Data Science - Roger D, Peng and Elizabeth Matsui
\item
  UC Berkeley R programming Guide .
\end{enumerate}


\end{document}
